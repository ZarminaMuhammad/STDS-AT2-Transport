# Load Libraries
library(dplyr)
library(xlsx)
library(lubridate)
library(ggplot2)
library(tidyr)
library(ggthemes)
library(stringr)
library(plyr)
library(data.table)
library(plotly)
library(Amelia)
library(plotrix)
library(RColorBrewer)
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
library("RCurl")
library("XML")
library(gridExtra)
#read phone usage data in
phone_usage <- read.xlsx("36100 - The Hungry 7 - Structured Data Collection.xlsx", sheetIndex = 8)
#find blank rows
missing_rows <- which(is.na(phone_usage$Date))
#getting rid of blank rows
phone_usage <- phone_usage[-missing_rows,]
phone_usage[is.na(phone_usage$Time..minutes.),]$Time..minutes. <- 0
#scale app usage %
phone_usage$Daily.App.Usage.. <- phone_usage$Daily.App.Usage.. * 100
#add weekday column
phone_usage$weekday <- weekdays(phone_usage$Date)
# EDA
start_date <-  "2019/03/20"
end_date <- "2019/04/28"
max_entries <- (as.numeric(difftime(ymd(end_date), ymd(start_date), units = "days")) + 1) * 5
max_days <- max_entries/5
#entries missing
entries <- phone_usage %>% group_by(Name) %>% summarise(number = n())
p1 <- ggplot(entries, aes(Name,number, fill= number < max_entries)) + geom_bar(stat = "identity") +
theme_economist() + scale_fill_economist(name = "Missing entries ",labels=c("Collected", "Missing")) +
ggtitle("Missing entries") +
ylab ("Number of entries (max 200)") +
xlab ("Team Members")
days<- phone_usage %>% group_by(Name) %>%  summarise(count=n_distinct(Date))
days$missing <- max_days - days$count
days <- gather(days, days, value, count:missing)
fill <- c("#5F9EA0", "#E1B378")
p2 <- ggplot(days, aes(Name,value, fill= days)) +
geom_bar(stat = "identity", position = position_fill(reverse = TRUE)) +
#geom_text(aes(label= value/40), vjust=1) +
theme_economist() + scale_fill_economist(name = "Missing entries ",labels=c("Collected", "Missing")) +
ggtitle("% of missing days") +
ylab ("Percent of data recorded in days") +
xlab ("Team Members")
grid.arrange(p1,p2,ncol = 2)
plot_data1 <- phone_usage %>% group_by(weekday) %>% summarise(mean = mean(Time..minutes.))
ggplot(plot_data1, aes(reorder(weekday,-mean), mean, fill = weekday))   +
geom_bar(stat = "identity") + theme_bw() +
geom_text(aes(label= round(mean,0)), vjust=-.5) +
xlab("Weekday") + ylab("Average phone usage in minutes")
plot_data_phone2 <- phone_usage %>% select(Name,App.Name, Time..minutes.) %>% group_by(Name,App.Name) %>%
summarise(Total = sum(Time..minutes.)) %>%
arrange(desc(Total)) %>% top_n(5)
ggplot(plot_data_phone2,aes(reorder(App.Name,-Total), Total, fill = Name)) +
geom_bar(stat="identity") +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
xlab("Applications") + ylab("Total time spent in Minutes")
#Part 3 tfidf
#same corpus as in previous exercise
##assumes tm, ggplot and wordcloud libraries are loaded
kableExtra::kable(c("a","b"))
#Part 3 tfidf
#same corpus as in previous exercise
##assumes tm, ggplot and wordcloud libraries are loaded
kableExtra::kable(c("a","b", "c"))
library(kableExtra)
#Part 3 tfidf
#same corpus as in previous exercise
##assumes tm, ggplot and wordcloud libraries are loaded
kable(c("a","b", "c"))
kable(dt)
library(knitr)
#Part 3 tfidf
#same corpus as in previous exercise
##assumes tm, ggplot and wordcloud libraries are loaded
kable(c("a","b", "c"))
dt <- mtcars[1:5, 1:6]
kable(dt)
kable(dt)
wordcountaddin:::text_stats()
library('wordcountaddin')
install.packages("wordcountaddin")
#install.packages(c("tidyverse","flexdashboard","shiny","knitcitations","bibtex","psych","devtools","curl","reshape2","tidyr","lattice","kfigr","rwunderground","gganimate"))
# devtools::install_github("cboettig/knitcitations")
#devtools::install_github("benmarwick/wordcountaddin", type = "source", dependencies = TRUE)
#go to Tools > Addins to select the wordcountaddin
library(knitcitations); cleanbib()
cite_options(citation_format = "pandoc", check.entries=FALSE)
library(bibtex)
library(psych)
library(curl)
library(devtools)
library(dplyr)
library(ggplot2)
library(psych)
library(tidyr)
library(reshape2)
library(knitr)
library(lattice) #just to illustrate another histogram function
library(kfigr) #this lets us crossreference figures, etc. Read more about it https://github.com/mkoohafkan/kfigr/edit/master/vignettes/introduction.Rmd
options("kfigr.prefix" = TRUE)
options("kfigr.link" = TRUE)
wordcountaddin:::text_stats()
# Sydney is 33.8688° S, 151.2093° E
#see https://github.com/ALShum/rwunderground - you'll need to register an api key
#use_metric = TRUE
end_date <- Sys.Date()-1
start_date <- Sys.Date()-35
#lookup_airport("Melbourne")
#set_location(zip_code = "2016") you can also use this
#weather_sydney <- history_range(set_location(airport_code = "SYD"), date_start = start_date, date_end = end_date, limit = 10, no_api = FALSE, use_metric = TRUE, key = get_api_key(), raw = FALSE, message = TRUE)
#write.csv(weather_sydney, file = "syd_weath.csv")
weather_sydney <- read.csv("syd_weath.csv", stringsAsFactors = F)
#weather_melbourne <- history_range(set_location(PWS_id = "IVICTORI842"), date_start = start_date, date_end = end_date, limit = 10, no_api = FALSE, use_metric = TRUE, key = get_api_key(), raw = FALSE, message = TRUE) #deliberately taking data from a slightly worse station. Use airport_code = "YMML" for equiv
#weather_melbourne_gd <- history_range(set_location(PWS_id = "INORTHCO3"), date_start = start_date, date_end = end_date, limit = 10, no_api = FALSE, use_metric = TRUE, key = get_api_key(), raw = FALSE, message = TRUE)
#write.csv(weather_melbourne, file = "mel_weath.csv")
weather_melbourne <- read.csv("mel_weath.csv", stringsAsFactors = F)
#to make this more interesting I'm going to randomly delete 500 observations
weather_melbourne <- weather_melbourne[-sample(1:nrow(weather_melbourne), 800), ]
#for another 250 observations we're going to deliberately add noisy missing data in the form of -9999 values
weather_melbourne$temp[sample(nrow(weather_melbourne),250)] <- -9999
#weather_sydney_summary <- history_daily(set_location(airport_code = "SYD"), date = start_date, use_metric = TRUE, key = get_api_key(), raw = FALSE, message = TRUE)
#weather_melbourne_summary <- history_daily(set_location(airport_code = "YMML"), date = start_date, use_metric = TRUE, key = get_api_key(), raw = FALSE, message = TRUE)
#if we wanted to write this data and read it. OR if you want to read data from your system or the web, you can use this pair of lines
#write.csv(weather_sydney, file = "syd_weath.csv")
#read.csv("syd_weath.csv", stringsAsFactors = F) #(you might wantto change stringsasfactors to True)
library(knitr)
kable(rbind(psych::describe(weather_sydney$temp),psych::describe(weather_melbourne$temp)), caption = "Summary of Mel & Sydney weather")
#note, you should label the rows
hist(weather_sydney$temp)
hist(weather_melbourne$temp)
#this data used to need a lot more cleaning! Previously, you need to remove -9999 values
SYD_temp <- as.data.frame(as.numeric(unlist(subset(weather_sydney, temp >-300, select=c("temp"))))) #you could also replace these wiht NA, but here we're just going to exclude the missing data
colnames(SYD_temp)[1] <- "temp"
SYD_temp$loc <- "SYD"
MEL_temp <- as.data.frame(as.numeric(unlist(subset(weather_melbourne, temp >-300, select=c("temp")))))
colnames(MEL_temp)[1] <- "temp"
MEL_temp$loc <- "MEL"
temps <- rbind(SYD_temp, MEL_temp)
temps$temp <- as.numeric(temps$temp)
ggplot(temps, aes(x = temp, fill = loc)) + geom_histogram(alpha = .5, position = 'identity')
word_cloud_text <- paste(youtube$fulltitle, youtube$category, youtube$Tags)
word_cloud_text <- gsub('["]', "",word_cloud_text)
wc <- Corpus(VectorSource(word_cloud_text))
wc <- tm_map(wc, tolower)
wc <- tm_map(wc, removeWords, stopwords("english"))
res<- wordcloud(wc, max.words=50,colors=brewer.pal(6,"Dark2"))
plot_data_yt3 <- youtube %>% select(Country) %>% na.omit() %>%
group_by(Country) %>% summarise(count = n())
ggplot(plot_data_yt3, aes(reorder(Country,-count), count, fill=as.factor(Country) )) +
guides(fill=guide_legend(title="Country")) +
geom_bar(stat = "identity") +
geom_text(aes(label= count), vjust=-1) +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
scale_y_continuous(limits=c(0,300)) +
xlab("Video Category") + ylab ("Frequency")
food_calorie$weekday <- weekdays(dmy(food_calorie$date))
plot_food_2 <- food_calorie %>% group_by(weekday) %>% summarise(mean=mean(Total))
ggplot(plot_food_2, aes(reorder(weekday,-mean), mean, fill = weekday)) +
geom_bar(stat = "identity") +
geom_text(aes(label= round(mean,0)), vjust=-.5) +
scale_y_continuous(limits=c(0,550)) +
ylab("Average calorie intake") + xlab ("Weekday")
food_calorie$weekday <- weekdays(dmy(food_calorie$date))
plot_food_2 <- food_calorie %>% group_by(weekday) %>% summarise(mean=mean(Total))
ggplot(plot_food_2, aes(reorder(weekday,-mean), mean, fill = weekday)) +
geom_bar(stat = "identity") +
geom_text(aes(label= round(mean,0)), vjust=-.5) +
scale_y_continuous(limits=c(0,550)) +
ylab("Average calorie intake") + xlab ("Weekday")
my_colors = brewer.pal(8, "Set2")
p1 <- ggplot(plot_data_yt1, aes(Group.1, x)) +
geom_line(colour = "lightblue" ,size = 1.5) +
#geom_bar(stat = "identity", position = position_fill(reverse = TRUE)) +
theme_economist()+
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
scale_x_date(date_labels="%d %b %y",date_breaks  ="1 day") +
geom_point(size=2, shape=22, colour="black", fill="white") +
geom_area(fill="red", alpha=.5) +
xlab("Days") + ylab ("Time in minutes")
youtube$weekday <- weekdays(dmy(youtube$watch_date))
plot_data_yt5 <- youtube %>% group_by(weekday) %>% summarise(mean=mean(duration))
p2 <- ggplot(plot_data_yt5, aes(reorder(weekday, -mean), mean, fill = weekday)) +
geom_bar(stat = "identity")
grid.arrange(p1,p2,ncol  =1 )
dt
dt %>%
kable() %>%
kable_styling()
dt <- c("a","b","c")
dt %>%
kable() %>%
kable_styling()
dt %>%
kable() %>%
kable_styling( , full_width = F)
dt <- cbind("a","b","c")
dt %>%
kable() %>%
kable_styling( , full_width = F)
tb <- cbind("Date",	"App", "Name",	"Time (minutes)",	"Daily App Usage %")
tb %>% kable() %>%
kable_styling( , full_width = F)
# Load Libraries
library(dplyr)
library(xlsx)
library(lubridate)
library(ggplot2)
library(tidyr)
library(ggthemes)
library(stringr)
library(plyr)
library(data.table)
library(plotly)
library(Amelia)
library(plotrix)
library(RColorBrewer)
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
library("RCurl")
library("XML")
library(gridExtra)
library(kableExtra)
# Load Libraries
library(dplyr)
library(xlsx)
library(lubridate)
library(ggplot2)
library(tidyr)
library(ggthemes)
library(stringr)
library(plyr)
library(data.table)
library(plotly)
library(Amelia)
library(plotrix)
library(RColorBrewer)
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
library("RCurl")
library("XML")
library(gridExtra)
library(kableExtra)
library(knitr)
tb <- cbind("Date",	"App", "Name",	"Time (minutes)",	"Daily App Usage %")
tb
tb <- cbind("Date",	"App", "Name",	"Time (minutes)",	"Daily App Usage %")
tb
tb <- cbind("Date",	"App", "Name",	"Time (minutes)",	"Daily App Usage %")
print (tb, row.names= F)
tb <- cbind("Date",	"App", "Name",	"Time (minutes)",	"Daily App Usage %")
print (tb, row.names= FALSE)
tb <- cbind("Date",	"App", "Name",	"Time (minutes)",	"Daily App Usage %")
colnames(tb)
tb <- as.data.frame(cbind("Date",	"App", "Name",	"Time (minutes)",	"Daily App Usage %"))
colnames(tb)
plot_data_yt3 <- youtube %>% select(Country) %>% na.omit() %>%
group_by(Country) %>% summarise(count = n())
ggplot(plot_data_yt3, aes(reorder(Country,-count), count, fill=as.factor(Country) )) +
guides(fill=guide_legend(title="Country")) +
geom_bar(stat = "identity") +
geom_text(aes(label= count), vjust=-1) +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
scale_y_continuous(limits=c(0,300)) +
xlab("Video Category") + ylab ("Frequency")
plot_data_yt3 <- youtube %>% select(Country) %>% na.omit() %>%
group_by(Country) %>% summarise(count = n())
ggplot(plot_data_yt3, aes(reorder(Country,-count), count, fill=as.factor(Country) )) +
guides(fill=guide_legend(title="Country")) +
geom_bar(stat = "identity") +
theme_bw() +
geom_text(aes(label= count), vjust=-1) +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
scale_y_continuous(limits=c(0,300)) +
xlab("Video Category") + ylab ("Frequency")
kable(head(phone_usage))
kable(head(phone_usage))
kable(head(phone_usage[,-weekday]))
kable(head(phone_usage[,-6]))
kable(head(phone_usage[,-7]))
kable(head(phone_usage[,1:5]))
kable(head(phone_usage[,1:5])) %>% kable_styling( , full_width = F)
kable(head(phone_usage[,1:5])) %>% kable_styling( )
kable(head(phone_usage[,1:5])) %>%  kable_styling(bootstrap_options=c("condensed","striped","bordered"), full_width=FALSE, position="left")
setwd("D:/UTS/DAM/Block Session 5 - Decks and Exercises")
setwd("D:/UTS/DAM/Block Session 5 - Decks and Exercises/textmining")
# run through the creation of the dtm again for practice
#
#Some additional tips:
# 1. Read the documentation for kmeans clustering algorithms:
#    https://stat.ethz.ch/R-manual/R-devel/library/stats/html/kmeans.html
#  2. There are many options for algorithms, we'll use the default (Hartigan Wong) method for kmeans. See
#     http://stackoverflow.com/questions/20446053/k-means-lloyd-forgy-macqueen-hartigan-wong
#     for details of the different methods if you're interested
# 3. Code assumes you are already in the working directory set in the earlier exercise
#
rm(list=ls())
dev.off()
library(tm)
docs <- VCorpus(DirSource("./docs-1"))
#Mac users only!!
#docs <- tm_map(docs, function(x) iconv(x, to='UTF-8-MAC', sub='byte'))
#Check number of docs loaded
print(docs)
#inspect a particular document
writeLines(as.character(docs[[30]]))
#Remove punctuation - replace punctuation marks with " "
docs <- tm_map(docs, removePunctuation)
#Transform to lower case
docs <- tm_map(docs,content_transformer(tolower))
#Strip digits
docs <- tm_map(docs, removeNumbers)
#Remove stopwords from standard stopword list
docs <- tm_map(docs, removeWords, stopwords("english"))
#Strip whitespace (cosmetic?)
docs <- tm_map(docs, stripWhitespace)
#inspect output
writeLines(as.character(docs[[30]]))
#Stem document
docs <- tm_map(docs,stemDocument)
#inspect
writeLines(as.character(docs[[30]]))
#Create document-term matrix
dtm <- DocumentTermMatrix(docs)
## start clustering specific code
#convert dtm to matrix (what format is the dtm stored in?)
m<-as.matrix(dtm)
#write as csv file
write.csv(m,file="dtmAsMatrix.csv")
#shorten rownames for display purposes
#rownames(m) <- paste(substring(rownames(m),1,7),rep("..",nrow(m)),
#                   substring(rownames(m),
#                               nchar(rownames(m))-7,nchar(rownames(m))-4))
#compute distance between document vectors
d <- dist(m)
#kmeans clustering
#kmeans - run with nstart=100 and k=2,3,5 to compare results with hclust
kfit <- kmeans(d, 2, nstart=100)
#plot - need library cluster
library(cluster)
clusplot(as.matrix(d), kfit$cluster, color=T, shade=T, labels=2, lines=0)
#print contents of kfit
print(kfit)
#print cluster sizes
kfit$size
#print clusters (members)
kfit$cluster
#write clusters to csv file
write.csv(kfit$cluster,file="KMClustGroups2.csv")
#sum of squared distance between cluster centers
kfit$betweenss
#sum of squared distance within a cluster (this are the quantities that the algorithm
#attempts to minimise)
kfit$withinss
#kmeans - how to determine optimal number of clusters?
#One approach: look for "elbow" in plot of summed intra-cluster distances (withinss) as fn of k
wss <- 2:(length(docs)-1)
for (i in 2:(length(docs)-1)) wss[i] <- sum(kmeans(d,centers=i,nstart=25)$withinss)
plot(2:(length(docs)-1), wss[2:(length(docs)-1)], type="b", xlab="Number of Clusters",
ylab="Within groups sum of squares")
#rerun using cosine distance
cosineSim <- function(x){
as.dist(x%*%t(x)/(sqrt(rowSums(x^2) %*% t(rowSums(x^2)))))
}
cs <- cosineSim(m)
cd <- 1-cs
kfit <- kmeans(cd, 40, nstart=100)
clusplot(as.matrix(cd), kfit$cluster, color=T, shade=T, labels=2, lines=0)
#print contents of kfit
print(kfit)
#print cluster sizes
kfit$size
#print clusters (members)
kfit$cluster
kfit <- kmeans(cd, 2, nstart=100)
clusplot(as.matrix(cd), kfit$cluster, color=T, shade=T, labels=2, lines=0)
#print contents of kfit
print(kfit)
#print cluster sizes
kfit$size
#print clusters (members)
kfit$cluster
#write clusters to csv file
write.csv(kfit$cluster,file="KMClustGroups2_cosine.csv")
#sum of squared distance between cluster centers (attempt to maximize)
kfit$betweenss
#sum of squared distance within a cluster (this are the quantities that the algorithm
#attempts to minimise)
kfit$withinss
#kmeans - how to determine optimal number of clusters?
#One approach: look for "elbow" in plot of summed intra-cluster distances (withinss) as fn of k
wss <- 2:(length(docs)-1)
for (i in 2:(length(docs)-1)) wss[i] <- sum(kmeans(cd,centers=i,nstart=25)$withinss)
plot(2:(length(docs)-1), wss[2:(length(docs)-1)], type="b", xlab="Number of Clusters",
ylab="Within groups sum of squares")
docs <- VCorpus(DirSource("./docs-1"))
#Mac users only!!
#docs <- tm_map(docs, function(x) iconv(x, to='UTF-8-MAC', sub='byte'))
#Check number of docs loaded
print(docs)
#inspect a particular document (another way to do this)
writeLines(as.character(docs[[30]]))
#Remove punctuation - replace punctuation marks with " "
docs <- tm_map(docs, removePunctuation)
#Transform to lower case
docs <- tm_map(docs,content_transformer(tolower))
#Strip digits
docs <- tm_map(docs, removeNumbers)
#Remove stopwords from standard stopword list
docs <- tm_map(docs, removeWords, stopwords("english"))
#Strip whitespace (cosmetic?)
docs <- tm_map(docs, stripWhitespace)
#inspect output
writeLines(as.character(docs[[30]]))
#Stem document
docs <- tm_map(docs,stemDocument)
#inspect
writeLines(as.character(docs[[30]]))
#Create document-term matrix
dtm <- DocumentTermMatrix(docs)
## start clustering specific code
#convert dtm to matrix (what format is the dtm stored in?)
m<-as.matrix(dtm)
#write as csv file
write.csv(m,file="dtmAsMatrix.csv")
#shorten rownames for display purposes
#rownames(m) <- paste(substring(rownames(m),1,3),rep("..",nrow(m)),
#                     substring(rownames(m),
#                               nchar(rownames(m))-12,nchar(rownames(m))-4))
#compute distance between document vectors
d <- dist(m)
#run hierarchical clustering using Ward's method (explore other options later)
groups <- hclust(d,method="ward.D")
#plot, use hang to ensure that labels fall below tree
plot(groups, hang=-1)
#cut into 2 subtrees. Try 3,4,5,6 cuts; comment on your results
rect.hclust(groups,2)
hclusters <- cutree(groups,2)
write.csv(hclusters,"hclusters.csv")
dev.off()
#try another distance measure
cosineSim <- function(x){
as.dist(x%*%t(x)/(sqrt(rowSums(x^2) %*% t(rowSums(x^2)))))
}
cs <- cosineSim(m)
cd <- 1-cs
#run hierarchical clustering using cosine distance
groups <- hclust(cd,method="ward.D")
#plot, use hang to ensure that labels fall below tree
plot(groups, hang=-1)
#cut into 2 subtrees.
rect.hclust(groups,2)
m
dtm
m
View(m)
setwd("~/GitHub/STDS-AT2-Transport")
##### Clear Workspace
rm(list=ls())
library(cluster)
#Clustering Algo to find similar LGAs
unified_new <- read.csv("clustering-input.csv")
unif <- unified_new
unif <- unif[-20,]
rownames(unif) <- unif[,1]
unif[,1] <- NULL
View(unif)
unif <- unified_new
unif <- unif[-20,]
rownames(unif) <- unif[,1]
unif[,1:3] <- NULL
#rfind cosine distance
cosineSim <- function(x){
as.dist(x%*%t(x)/(sqrt(rowSums(x^2) %*% t(rowSums(x^2)))))
}
# calculate cosine distance
cs <- cosineSim(as.matrix(unif))
cd <- 1-cs
#create 5 clusters
kfit <- kmeans(cd, 5, nstart=100)
#plot the clusters
clusplot(as.matrix(cd), kfit$cluster, color=T, shade=T, labels=2, lines=0)
#write clusters to csv file
write.csv(kfit$cluster,file="KMClustGroups2_cosine.csv")
# loop to find optimal number of clusters
wss <- 2:(nrow(unif)-1)
for (i in 2:(nrow(unif)-1)) wss[i] <- sum(kmeans(cd,centers=i,nstart=25)$withinss)
plot(2:(nrow(unif)-1), wss[2:(nrow(unif)-1)], type="b", xlab="Number of Clusters",
ylab="Within groups sum of squares")
#run hierarchical clustering using cosine distance
groups <- hclust(cd,method="ward.D")
#plot, use hang to ensure that labels fall below tree
plot(groups, hang=-1)
#cut into 2 subtrees.
rect.hclust(groups,5)
hclusters_cosine <- cutree(groups,5)
write.csv(hclusters_cosine,"hclusters_cosine.csv")
